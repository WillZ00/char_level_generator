{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZDZrbIbALEf",
        "outputId": "179b74e1-ae8d-4e4a-9f3b-eabeb0beb78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import shuffle, seed, choice\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split,Dataset,DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = 'sample_data/'\n",
        "\n",
        "concatenated_content = ''\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        with open(file_path, 'r') as file:\n",
        "            concatenated_content += file.read() + '\\n'\n",
        "\n",
        "print(f\"Total number of characters in concatenated files: {len(concatenated_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esk9F96qAWEQ",
        "outputId": "6f3cc25a-0f03-45ec-b5d7-f71c649949f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters in concatenated files: 116615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_code(code):\n",
        "    code = re.sub(r'(?<=[^\\n])  +', ' ', code)\n",
        "    code = '\\n'.join(line.strip() for line in code.splitlines())\n",
        "    return code\n"
      ],
      "metadata": {
        "id": "JcSGeBhMAjlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_content = preprocess_code(concatenated_content)"
      ],
      "metadata": {
        "id": "Znr4Du6dAoZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(concatenated_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh6ho1VdApxs",
        "outputId": "5be44f2d-75e6-46e6-faa5-ac3ce5c5ebc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "import java.util.Random;\n",
            "\n",
            "public class HashTableExperiment {\n",
            "\n",
            "private static int count = 0;\n",
            "\n",
            "\n",
            "\n",
            "/******************\n",
            "* Helper Methods *\n",
            "******************/\n",
            "private static boolean less(int v, int w) {\n",
            "++count;\n",
            "return (v < w);\n",
            "}\n",
            "\n",
            "private static boolean equal(int v, int w) {\n",
            "++count;\n",
            "return (v == w);\n",
            "}\n",
            "\n",
            "private class Node {\n",
            "int item;\n",
            "Node next;\n",
            "public Node(int k, Node n) {\n",
            "item = k;\n",
            "next = n;\n",
            "}\n",
            "\n",
            "}\n",
            "\n",
            "/***************************\n",
            "* Our methods for the lab *\n",
            "***************************/\n",
            "\n",
            "private static Random rnd = new Random();\n",
            "\n",
            "public static int[] genRandomArray(int size){\n",
            "\n",
            "int a[] = new int[size];\n",
            "for(int i = 0; i < a.length; i++){\n",
            "a[i] = rnd.nextInt();\n",
            "}\n",
            "return a;\n",
            "}\n",
            "\n",
            "public static int[] genSortedArray(int size){\n",
            "int a[] = new int[size];\n",
            "for(int i = 0; i < a.length; i++){\n",
            "a[i] = i;\n",
            "}\n",
            "return a;\n",
            "}\n",
            "\n",
            "// create a best case tree for the integers 1 .. N, by doing binary search exhaustively\n",
            "// on all ints and entering nodes into the tree\n",
            "\n",
            "private static Node genBestBST(int N) {\n",
            "return genBestBSTHel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = max(20000, len(concatenated_content))\n",
        "text = concatenated_content[:size]\n",
        "\n",
        "chars_in_text = sorted(list(set(text)))\n",
        "char2int = {ch: i for i, ch in enumerate(chars_in_text)}\n",
        "int2char = {i: ch for i, ch in enumerate(chars_in_text)}\n",
        "num_chars = len(chars_in_text)\n",
        "\n",
        "sample_len = 300\n",
        "input_seq_chars = []\n",
        "target_seq_chars = []\n",
        "\n",
        "for i in range(0, len(text) - sample_len, 1):\n",
        "    input_seq_chars.append(text[i:i + sample_len - 1])\n",
        "    target_seq_chars.append(text[i + 1:i + sample_len])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "LCIcZYxnAshF",
        "outputId": "73fb6a0d-dfad-4b07-d193-a0032490029d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'concatenated_content' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-697881363f17>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenated_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchars_in_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchar2int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars_in_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'concatenated_content' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = [[char2int[ch] for ch in seq] for seq in input_seq_chars]\n",
        "target_seq = [[char2int[ch] for ch in seq] for seq in target_seq_chars]\n",
        "\n",
        "def seq2OneHot(seq, num_chars):\n",
        "    return np.array([np.eye(num_chars)[s] for s in seq], dtype=np.float32)\n",
        "\n",
        "input_seq = seq2OneHot(input_seq, num_chars)\n",
        "target_seq = seq2OneHot(target_seq, num_chars)\n",
        "\n",
        "input_seq = torch.tensor(input_seq, dtype=torch.float32)\n",
        "target_seq = torch.tensor(target_seq, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "-QwAHhwPAxFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq.shape, target_seq.shape"
      ],
      "metadata": {
        "id": "WZ0_1EmNA0dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "3GIy094lA2w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Basic_Dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "ds = Basic_Dataset(input_seq, target_seq)\n",
        "batch_size = 128\n",
        "data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "tB1bLLmXA5gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "\n",
        "hidden_dim = 128\n",
        "n_layers = 1\n",
        "dropout = 0.1\n",
        "model = Model(num_chars, num_chars, hidden_dim, n_layers, dropout).to(device)"
      ],
      "metadata": {
        "id": "Jx9yNNfsBAvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
      ],
      "metadata": {
        "id": "ZX_WJwYQBBmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "losses = []\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.train()\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for input_seq_batch, target_seq_batch in data_loader:\n",
        "        input_seq_batch, target_seq_batch = input_seq_batch.to(device), target_seq_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        target_seq_hat = model(input_seq_batch)\n",
        "        loss = loss_fn(target_seq_hat, target_seq_batch.view(-1, num_chars))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "# Plotting the loss\n",
        "plt.title('Loss over epochs')\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qthuu2NSBD8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def int2OneHot(X,size):\n",
        "\n",
        "    def int2OneHot1(x,size=10):\n",
        "        tmp = np.zeros(size)\n",
        "        tmp[int(x)] = 1.0\n",
        "        return tmp\n",
        "\n",
        "    return np.array([ int2OneHot1(x, size) for x in X ]).astype('float32')\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    softmax = F.softmax(scaled_logits, dim=0)\n",
        "    return softmax.detach().cpu().numpy()\n",
        "\n",
        "def predict(model, ch, temperature=0.3):\n",
        "    ch = ch[-(sample_len - 1):]\n",
        "    ch = np.array([char2int[c] for c in ch])\n",
        "    ch = np.array([int2OneHot(ch, num_chars)])\n",
        "    ch = torch.from_numpy(ch).to(device)\n",
        "    out = model(ch)\n",
        "    prob = softmax_with_temperature(out[-1], temperature)\n",
        "    char_ind = np.random.choice(list(range(num_chars)), p=prob)\n",
        "    return int2char[char_ind]\n",
        "\n",
        "def sample(model, out_len, start, temperature=0.3):\n",
        "    model.eval()\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    for _ in range(size):\n",
        "        char = predict(model, ''.join(chars), temperature)\n",
        "        chars.append(char)\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "AVKcNGcFBI_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmVF_arDIgW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_code = sample(model, 300, \"import java.awt.Color;\\nimport java.utils.Random;\\n public \", temperature=0.3)\n",
        "print(generated_code)"
      ],
      "metadata": {
        "id": "S3iWO_q4BJg5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}